<<<<<<< HEAD
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
</head>
<body>
      <div class="px-6 pb-6 flex-1">
        <div
          class="w-full text-[#007bff] font-normal text-[19.2px]"
          style="font-family: Raleway, sans-serif"
        ><p>In signal processing, basic matrices play crucial roles in various operations, forming the backbone of many algorithms and techniques. The identity matrix, a specific type of diagonal matrix, serves as a neutral element in <span><a href ="./theory/Properties.pdf">matrix multiplication</a></span>, crucial for preserving original signal values during transformations. Diagonal matrices streamline computations by allowing for independent scaling of signal components. <span><a href ="./theory/Singular_Value_Decomposition.pdf">Singular Value Decomposition (SVD)</a></span> is a powerful tool in signal processing, extensively applied in noise reduction, data compression, and system identification. Matrix multiplication is fundamental in filtering, convolution, and applying linear transformations to signals, with convolution matrices specifically used to represent the effect of filters or kernels on signals.</p>
        <p>The Fourier matrix is essential for converting signals between time and frequency domains in signal processing. <span><a href ="./theory/Properties.pdf">Transposition</a></span> is used in forming cross-correlation matrices and working with orthogonal bases, like in PCA. Determinants help analyze system stability and calculate volumes in multivariate Gaussian distributions, while <span><a href ="./theory/EigenValue_and_EigenVector.pdf">eigenvalue decomposition</a></span> simplifies the representation of systems and signal energy distribution.</p>
        <p><span><a href ="./theory/LU_Decomposition.html">LU decomposition</a></span> is essential for solving linear equations, inverting matrices, and computing determinants. In signal processing, linear equations are crucial for various tasks, including filter design, system identification, and signal reconstruction. They are used to model and solve problems such as noise reduction, linear filtering, and Fourier transforms. Linear systems also underpin techniques like Principal Component Analysis (PCA) and Kalman filtering for estimation and prediction. Essentially, they provide the mathematical framework for manipulating and analyzing signals efficiently.</p>
        <p><span><a href ="./theory/Row_Echelon_Form.html">Row echelon form</a></span> is a fundamental tool in linear algebra that supports various signal processing applications by simplifying the analysis and solution of linear systems, determining matrix rank, etc.</p>
        <p>These foundational matrices enable efficient computation, representation, and manipulation of signals, serving as the building blocks for more complex signal processing tasks.</p>
        </div></div>
<div class="px-6 pb-6 flex-1">
  <div
    class="w-full text-[#007bff] font-normal text-[19.2px]"
    style="font-family: Raleway, sans-serif"
  >
    <ol class="mb-4 pl-4">
      <li>
        <a href="./theory/Properties.pdf">
          <div class="flex">
            <span class="text-black mr-4">1.</span>
            <p class="hover:text-[#3e6389] hover:underline">
              Properties of Matrix Operations (Addition, Multiplication, Transpose, Determinants, Minor, Co-factor, Adjoint, Inverse, etc.)
            </p>
          </div>
        </a>
      </li>
      <li>
        <a href="./theory/EigenValue_and_EigenVector.pdf">
          <div class="flex">
            <span class="text-black mr-4">2.</span>
            <p class="hover:text-[#3e6389] hover:underline">
              Eigenvalue and EigenVector
            </p>
          </div>
        </a>
      </li>
      <li>
        <a href="./theory/Singular_Value_Decomposition.pdf">
          <div class="flex">
            <span class="text-black mr-4">3.</span>
            <p class="hover:text-[#3e6389] hover:underline">
              Singular Value Decomposition (SVD)
            </p>
          </div>
        </a>
      </li>
      <li>
        <a href="./theory/LU_Decomposition.html">
          <div class="flex">
            <span class="text-black mr-4">4.</span>
            <p class="hover:text-[#3e6389] hover:underline">
              LU Decomposition Method
            </p>
          </div>
        </a>
      </li>
      <li>
        <a href="./theory/Row_Echelon_Form.html">
          <div class="flex">
            <span class="text-black mr-4">5.</span>
            <p class="hover:text-[#3e6389] hover:underline">
              Row Echelon Form
            </p>
          </div>
        </a>
      </li>
      <li>
        <a href="./theory/Reduced_Row_Echelon_Form.html">
          <div class="flex">
            <span class="text-black mr-4">6.</span>
            <p class="hover:text-[#3e6389] hover:underline">
              Reduced Row Echelon Form
            </p>
          </div>
        </a>
      </li>
      <li>
        <a href="./theory/Rank.pdf">
          <div class="flex">
            <span class="text-black mr-4">7.</span>
            <p class="hover:text-[#3e6389] hover:underline">
              Rank of a Matrix
            </p>
          </div>
        </a>
      </li>
    </ol>
  </div>
</div>


</body>
</html>
=======
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
</head>
<body>
      <div class="px-6 pb-6 flex-1">
        <div
          class="w-full text-[#007bff] font-normal text-[19.2px]"
          style="font-family: Raleway, sans-serif"
        ><p>In signal processing, basic matrices play crucial roles in various operations, forming the backbone of many algorithms and techniques. The identity matrix, a specific type of diagonal matrix, serves as a neutral element in matrix multiplication, crucial for preserving original signal values during transformations. Diagonal matrices streamline computations by allowing for independent scaling of signal components. Singular Value Decomposition (SVD) is a powerful tool in signal processing, extensively applied in noise reduction, data compression, and system identification. Matrix multiplication is fundamental in filtering, convolution, and applying linear transformations to signals, with convolution matrices specifically used to represent the effect of filters or kernels on signals.</p>
        <p>The Fourier matrix is essential for converting signals between time and frequency domains in signal processing. Transposition is used in forming cross-correlation matrices and working with orthogonal bases, like in PCA. Determinants help analyze system stability and calculate volumes in multivariate Gaussian distributions, while eigenvalue decomposition simplifies the representation of systems and signal energy distribution.</p>
        <p>LU decomposition is essential for solving linear equations, inverting matrices, and computing determinants. In signal processing, linear equations are crucial for various tasks, including filter design, system identification, and signal reconstruction. They are used to model and solve problems such as noise reduction, linear filtering, and Fourier transforms. Linear systems also underpin techniques like Principal Component Analysis (PCA) and Kalman filtering for estimation and prediction. Essentially, they provide the mathematical framework for manipulating and analyzing signals efficiently.</p>
        <p>Row echelon form is a fundamental tool in linear algebra that supports various signal processing applications by simplifying the analysis and solution of linear systems, determining matrix rank, etc.</p>
        <p>These foundational matrices enable efficient computation, representation, and manipulation of signals, serving as the building blocks for more complex signal processing tasks.</p>
        </div></div>
<br/><br/>
<p>
				<strong>Properties of Matrix Operations</strong><a></a>
			</p>
			<p>
				<strong>Properties</strong><strong>&#xa0;</strong><strong>of Addition</strong>
			</p>
			<p>
				The basic properties of addition for real numbers also hold true for matrices.&#xa0;
			</p>
			<p>
				Let&#xa0;A,&#xa0;B&#xa0;and&#xa0;C&#xa0;be&#xa0;m x n&#xa0;matrices
			</p>
			<ol>
				<li>
					A + B&#xa0; =&#xa0; B + A&#xa0;&#xa0;&#xa0;&#xa0;commutative
				</li>
				<li>
					A + (B + C)&#xa0; =&#xa0; (A + B) + C&#xa0;&#xa0;&#xa0;&#xa0;associative
				</li>
				<li>
					There is a unique&#xa0;m x n&#xa0;matrix&#xa0;O&#xa0;with&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;A + O&#xa0; =&#xa0; A&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;additive identity
				</li>
				<li>
					For any&#xa0;&#xa0;m x n&#xa0;matrix&#xa0;A&#xa0;there is an&#xa0;m x n&#xa0;matrix&#xa0;B&#xa0;(called&#xa0;-A) with&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;A + B&#xa0; =&#xa0; O&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;additive inverse
				</li>
			</ol>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Properties of Matrix</strong><strong>&#xa0;</strong><a><strong>Multiplication</strong></a>
			</p>
			<p>
				Unlike matrix addition, the properties of multiplication of real numbers do not all generalize to matrices.&#xa0; Matrices rarely commute even if&#xa0;AB&#xa0;and&#xa0;BA&#xa0;are both defined.&#xa0; There often is no multiplicative inverse of a matrix, even if the matrix is a square matrix.&#xa0; There are a few properties of multiplication of real numbers that generalize to matrices.&#xa0; We state them now.
			</p>
			<p>
				Let&#xa0;A,&#xa0;B&#xa0;and&#xa0;C&#xa0;be matrices of dimensions such that the following are defined.&#xa0; Then
			</p>
			<ol>
				<li>
					A(BC)&#xa0; =&#xa0; (AB)C&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0; associative
				</li>
				<li>
					A(B + C)&#xa0; =&#xa0; AB + AC&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;distributive
				</li>
				<li>
					(A + B)C&#xa0; =&#xa0; AC + BC&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;distributive
				</li>
				<li>
					There are unique matrices&#xa0;I<sub>m</sub>&#xa0;and&#xa0;I<sub>n</sub>&#xa0;with&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0; I<sub>m</sub>&#xa0;A&#xa0; =&#xa0; A I<sub>n</sub>&#xa0; =&#xa0; A&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;multiplicative identity
				</li>
			</ol>
			<p>
				<strong>&#xa0;</strong>
			</p>
			<p>
				<strong>Properties of</strong><strong>&#xa0;</strong><a><strong>Scalar</strong></a><strong>&#xa0;</strong><strong>Multiplication</strong>
			</p>
			<p>
				Since we can multiply a matrix by a scalar, we can investigate the properties that this multiplication has.&#xa0; All of the properties of multiplication of real numbers generalize.&#xa0; In particular, we have
			</p>
			<p>
				Let&#xa0;r&#xa0;and&#xa0;s&#xa0;be real numbers and&#xa0;A&#xa0;and&#xa0;B&#xa0;be matrices.&#xa0; Then
			</p>
			<ol>
				<li>
					r(sA)&#xa0; =&#xa0; (rs)A&#xa0;
				</li>
				<li>
					(r + s)A&#xa0; =&#xa0; rA + sA
				</li>
				<li>
					r(A + B)&#xa0; =&#xa0; rA + rB
				</li>
				<li>
					A(rB)&#xa0; =&#xa0; r(AB)&#xa0; =&#xa0; (rA)B
				</li>
			</ol>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Properties of the</strong><strong>&#xa0;</strong><a><strong>Transpose</strong></a><strong>&#xa0;</strong><strong>of a Matrix</strong>
			</p>
			<p>
				Recall that the transpose of a matrix is the operation of switching rows and columns.&#xa0; We state the following properties.&#xa0; We proved the first property in the last section.
			</p>
			<p>
				Let&#xa0;r&#xa0;be a real number and&#xa0;A&#xa0;and&#xa0;B&#xa0;be matrices.&#xa0; Then
			</p>
			<ol>
				<li>
					(A<sup>T</sup>)<sup>T</sup>&#xa0; =&#xa0; A
				</li>
				<li>
					(A + B)<sup>T</sup>&#xa0; =&#xa0; A<sup>T</sup>&#xa0;+ B<sup>T</sup>
				</li>
				<li>
					(AB)<sup>T</sup>&#xa0; =&#xa0; B<sup>T</sup>A<sup>T</sup>
				</li>
				<li>
					(rA)<sup>T</sup>&#xa0; =&#xa0; rA<sup>T</sup>
				</li>
			</ol>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Properties of Determinants</strong>
			</p>
			<ol>
				<li>
					det(A) = &#xa0;det(AT)
				</li>
				<li>
					If any row or column of a determinant, is multiplied by any scalar value, that is, a non-zero constant, the entire determinant gets multiplied by the same scalar, that is, if any row or column is multiplied by constant k, the determinant value gets multiplied by k.
				</li>
			</ol>
			<p>
				&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0; det(Δ’) = k det(Δ)
			</p>
			<ol start="3">
				<li>
					If all elements of any column or row are zero, then the determinant is zero
				</li>
				<li>
					If all the elements in the determinant above or below the diagonal are zero, then the determinant is a product of diagonal elements
				</li>
			</ol>
			<p>
				<strong>&#xa0;</strong>
			</p>
			<p>
				<strong>Minor, Co-factor, Adjoint, Inverse</strong>
			</p>
			<p>
				[A]<sub>(i x k) </sub><strong>.</strong> [B]<sub>(k x j)</sub>
			</p>
			<p>
				=<img src="1739254504_matrix-theory/1739254504_matrix-theory-1.png" width="226" height="68" alt="" />
			</p>
			<p>
				=<img src="1739254504_matrix-theory/1739254504_matrix-theory-2.png" width="110" height="68" alt="" /> = [C]<sub>ij</sub>
			</p>
			<p>
				Where, c<sub>ij</sub> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-3.png" width="19" height="21" alt="" /> a<sub>ik</sub>b<sub>kj</sub>
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				C<sup>T</sup> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-4.png" width="110" height="68" alt="" /> = [C]<sub>ji</sub>
			</p>
			<p>
				Where, C<sup>T </sup>is transpose of matrix C
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>1. a.)</strong>
			</p>
			<p>
				<strong>(A.B)</strong><strong><sup>T</sup></strong><strong> = B</strong><strong><sup>T</sup></strong><strong>.A</strong><strong><sup>T</sup></strong>
			</p>
			<p>
				(A.B)<sup>T</sup><sub>ij </sub>
			</p>
			<p>
				= (A.B)<sub>ji</sub>
			</p>
			<p>
				= a<sub>jk</sub>b<sub>ki</sub>
			</p>
			<p>
				= b<sub>ki</sub>a<sub>jk</sub>
			</p>
			<p>
				= (b<sup>T </sup>)<sub>ik </sub>(a<sup>T</sup>)<sub>kj</sub>
			</p>
			<p>
				= (B<sup>T</sup>A<sup>T</sup>)<sub>ij</sub>
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>(1. b.)</strong>
			</p>
			<p>
				<strong>B</strong><strong><sup>T</sup></strong><strong>.A</strong><strong><sup>T </sup></strong><strong>= (A.B)</strong><strong><sup>T</sup></strong>
			</p>
			<p>
				&#xa0;(B<sup>T</sup>A<sup>T</sup>)<sub>ij</sub>
			</p>
			<p>
				= (b<sup>T</sup>)<sub>ik</sub><sub>&#xa0; </sub><sub>.</sub><sub>&#xa0; </sub>(a<sup>T</sup>)<sub>kj</sub>
			</p>
			<p>
				= (b)<sub>ki</sub><sub>&#xa0; </sub><sub>.</sub><sub>&#xa0; </sub>(a)<sub>jk</sub>
			</p>
			<p>
				= (a)<sub>jk</sub> . (b)<sub>ki</sub><sub>&#xa0;&#xa0;&#xa0; </sub>
			</p>
			<p>
				= (AB)<sub>ji</sub>
			</p>
			<p>
				= (AB)<sup>T</sup><sub>ij</sub>
			</p>
			<p>
				<sub>&#xa0;</sub>
			</p>
			<p>
				Cofactor Matrix of a matrix A
			</p>
			<p>
				[CO<sub>ij</sub>] =&#xa0; (-1)<sup>(i+j) </sup>*<img src="1739254504_matrix-theory/1739254504_matrix-theory-5.png" width="123" height="68" alt="" />
			</p>
			<p>
				Where, <img src="1739254504_matrix-theory/1739254504_matrix-theory-6.png" width="274" height="68" alt="" />
			</p>
			<p>
				M<sub>ij </sub>is the entry in the i<sup>th</sup> row and j<sup>th</sup> column
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				Adjoint of a matrix, A
			</p>
			<p>
				Adj(A) = [CO<sub>ij</sub>]<sup>T</sup>
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				Now, the inverse of the matrix A
			</p>
			<p>
				A<sup>-1</sup> = Adj(A) / det(A)
			</p>
			<p>
				Where ‘det’ denotes the determinant
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>2. (AB)</strong><strong><sup>−1</sup></strong><strong>=B</strong><strong><sup>−1</sup></strong><strong>A</strong><strong><sup>−1</sup></strong>
			</p>
			<p>
				If A and B are invertible matrices or Both A and B are <strong>n X n</strong> square matrices and determinants are not zeroes then
			</p>
			<p>
				(AB)(AB)<sup>-1 </sup>= I 
			</p>
			<p>
				Where I is the identity matrix of size <strong>n X n</strong>
			</p>
			<p>
				&#xa0;Pre-multiply by A<sup>-1</sup>
			</p>
			<p>
				(A<sup>-1 </sup>). (AB)(AB)<sup>-1</sup> = A<sup>-1</sup>I
			</p>
			<p>
				Or, I.(B).(AB)<sup>-1</sup> = A<sup>-1</sup>
			</p>
			<p>
				Or,&#xa0; (B).(AB)<sup>-1 </sup>= A<sup>-1</sup>
			</p>
			<p>
				<sup>&#xa0;</sup>
			</p>
			<p>
				Pre-multiply by B<sup>-1</sup>
			</p>
			<p>
				(B<sup>-1 </sup>). (B).(AB)<sup>-1 </sup>= B<sup>-1</sup>A<sup>-1</sup>
			</p>
			<p>
				Or, I(AB)<sup>-1 </sup>= B<sup>-1</sup>A<sup>-1</sup>
			</p>
			<p>
				Or, (AB)<sup>-1 </sup>= B<sup>-1</sup>A<sup>-1</sup>
			</p>
			<p>
				<strong>&#xa0;</strong>
			</p>
			<p>
				<strong>3. Adj(A . B) = Adj(B) . Adj(A)</strong>
			</p>
			<p>
				(AB)<sup>−1</sup>=adj(AB)/det(AB)
			</p>
			<p>
				Or, adj(AB) = (AB)<sup>−1</sup>⋅det(AB)&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0; . . . (1)
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				It is also known that, (AB)<sup>−1</sup>=B<sup>-1</sup>A<sup>−1</sup>
			</p>
			<p>
				And, det(AB)=det(A)⋅det(B)&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0; . . . (2)
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				Also
			</p>
			<p>
				A<sup>−1</sup>=adj(A)/det(A) 
			</p>
			<p>
				B<sup>−1</sup>=adj(B)/det(B)
			</p>
			<p>
				Or, adj(A)=A<sup>-1</sup>det(A) 
			</p>
			<p>
				Or, adj(B)=B<sup>−1</sup>det(B)
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				adj(B)⋅adj(A)=detA⋅detB⋅B<sup>−1</sup>⋅A<sup>−1</sup>&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0; . . . (3)
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				Putting (2) in equation (1)
			</p>
			<p>
				adj(AB)=det(A)⋅det(B)⋅B<sup>−1</sup>⋅A<sup>−1</sup>&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0; . . . (4)
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				From (3) and (4)
			</p>
			<p>
				adj(AB)=adj(B)⋅adj(A)
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Eigenvalue and Eigenvector</strong>
			</p>
			<p>
				Let’s assume a square matrix <strong>A</strong>
			</p>
			<p>
				The characteristic equation,
			</p>
			<p>
				<strong>| A – </strong>λ*<strong>I | = </strong>0&#xa0;&#xa0; 
			</p>
			<p>
				<em>(where </em><strong><em>I </em></strong><em>is an identity matrix)</em>
			</p>
			<p>
				After calculating the values of λs we attempt to find eigenvectors for corresponding eigenvalues like this
			</p>
			<p>
				For eigenvalue, λ = λ<sub>1</sub>
			</p>
			<p>
				<strong>A*x</strong> = λ<sub>1</sub>*<strong>I*x</strong><strong>&#xa0;&#xa0; </strong><em>(where, </em><strong><em>x</em></strong><em> is an unknown vector)</em>
			</p>
			<p>
				Or, (<strong>A</strong> - λ<sub>1</sub>*<strong>I</strong>)*<strong>x</strong> = 0
			</p>
			<p>
				The value of <strong>x </strong>is the corresponding eigenvector of λ<sub>1</sub>
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Power Method for Dominant Eigenvalue</strong>
			</p>
			<p>
				Let λ<sub>1</sub>, λ<sub>2</sub>, λ<sub>3</sub>, and λ<sub>n</sub> be the eigenvalues of an <strong>n X n </strong>matrix <strong>A</strong>.&#xa0; λ<sub>1 </sub>is called the dominant eigenvalue of <strong>A</strong> if
			</p>
			<p>
				| λ<sub>1</sub>| &gt; | λ<sub>i </sub>|,&#xa0;&#xa0; <em>i = 2, 3, ... , n</em>
			</p>
			<p>
				The eigenvectors corresponding to λ<sub>1 </sub>are called dominant eigenvectors of <strong>A</strong>.
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Procedure</strong>
			</p>
			<ol>
				<li>
					Choose an <strong>n X n </strong>matrix
				</li>
			</ol>
			<p>
				<strong><em>The number of rows and columns should be the same (or matrix dimension mismatched)</em></strong>
			</p>
			<p>
				&#xa0;
			</p>
			<ol start="2">
				<li>
					Like the Jacobi and Gauss-Seidel methods, the power method for approximating eigenvalues is iterative. First, we assume that matrix <strong>A</strong> has a dominant eigenvalue with corresponding dominant eigenvectors. Then we choose an initial approximation <strong>x</strong><strong><sub>0</sub></strong> of one of the
				</li>
			</ol>
			<p>
				dominant eigenvectors of <strong>A</strong>. This initial approximation must be a nonzero vector in R<em><sup>n </sup></em>
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				Finally, we form the sequence given by
			</p>
			<p>
				<strong>x</strong><strong><sub>1</sub></strong><strong> = Ax</strong><strong><sub>0</sub></strong>
			</p>
			<p>
				<strong>x</strong><strong><sub>2</sub></strong><strong> = Ax</strong><strong><sub>1 </sub></strong><strong>= A(Ax</strong><strong><sub>0</sub></strong><strong>) = A</strong><strong><sup>2</sup></strong><strong>x</strong><strong><sub>0</sub></strong>
			</p>
			<p>
				<strong>x</strong><strong><sub>3</sub></strong><strong> = Ax</strong><strong><sub>2 </sub></strong><strong>= A(A</strong><strong><sup>2</sup></strong><strong>x</strong><strong><sub>0</sub></strong><strong>) = A</strong><strong><sup>3</sup></strong><strong>x</strong><strong><sub>0</sub></strong>
			</p>
			<p>
				<strong><sub>. . .</sub></strong>
			</p>
			<p>
				<strong>x</strong><strong><sub>n</sub></strong><strong> = Ax</strong><strong><sub>n-1 </sub></strong><strong>= A(A</strong><strong><sup>n-1</sup></strong><strong>x</strong><strong><sub>0</sub></strong><strong>) = A</strong><strong><sup>n</sup></strong><strong>x</strong><strong><sub>0</sub></strong>
			</p>
			<p>
				<em>(In the above, </em><strong><em>x</em></strong><strong><em><sub>1 </sub></em></strong><em>denotes</em><strong><em> </em></strong><em>the</em><strong><em> </em></strong><em>value of vector </em><strong><em>x </em></strong><em>at the first iteration and so on)</em>
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				Compare the updated value of <strong>x </strong>with its previous value (obtained from the previous iteration)
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				For large powers of k, and by properly scaling this sequence, we will see that we obtain a good approximation of the dominant eigenvector of <strong>A</strong>. 
			</p>
			<p>
				&#xa0;
			</p>
			<ol start="3">
				<li>
					Repeat the iteration process until convergence
				</li>
			</ol>
			<p>
				&#xa0;
			</p>
			<ol start="4">
				<li>
					The formula for finding the corresponding eigenvalue from eigenvector x.
				</li>
			</ol>
			<p>
				If <strong>x </strong>is an eigenvector of <strong>A</strong>, then its corresponding eigenvalue is given by 
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				λ = (<strong>Ax.x / x.x</strong>)
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				&#xa0;
			</p>
			<ol start="5">
				<li>
					If they do not converge even after many iterations (maybe after 1000 iterations), then
				</li>
			</ol>
			<p>
				<strong><em>Entered matrix has no dominant eigenvalue</em></strong>
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Example</strong>
			</p>
			<p>
				A = <img src="1739254504_matrix-theory/1739254504_matrix-theory-7.png" width="82" height="42" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				We begin with an initial nonzero approximation of
			</p>
			<p>
				<strong>x</strong><strong><sub>0</sub></strong> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-8.png" width="24" height="42" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				We then obtain the following approximations
			</p>
			<p>
				<strong>x</strong><strong><sub>1</sub></strong><strong> = Ax</strong><strong><sub>0 </sub></strong><strong>= </strong><img src="1739254504_matrix-theory/1739254504_matrix-theory-9.png" width="106" height="42" alt="" /> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-10.png" width="53" height="42" alt="" /> = -4<img src="1739254504_matrix-theory/1739254504_matrix-theory-11.png" width="49" height="42" alt="" />
			</p>
			<p>
				<strong>&#xa0;</strong>
			</p>
			<p>
				<strong>x</strong><strong><sub>2</sub></strong><strong> = Ax</strong><strong><sub>1 </sub></strong><strong>= </strong><img src="1739254504_matrix-theory/1739254504_matrix-theory-12.png" width="134" height="42" alt="" /> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-13.png" width="35" height="42" alt="" /> = 10<img src="1739254504_matrix-theory/1739254504_matrix-theory-14.png" width="49" height="42" alt="" />
			</p>
			<p>
				<strong>x</strong><strong><sub>3</sub></strong><strong> = Ax</strong><strong><sub>2 </sub></strong><strong>= </strong><img src="1739254504_matrix-theory/1739254504_matrix-theory-15.png" width="116" height="42" alt="" /> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-16.png" width="53" height="42" alt="" /> = -22<img src="1739254504_matrix-theory/1739254504_matrix-theory-17.png" width="49" height="42" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>x</strong><strong><sub>4</sub></strong><strong> = Ax</strong><strong><sub>3 </sub></strong><strong>= </strong><img src="1739254504_matrix-theory/1739254504_matrix-theory-18.png" width="134" height="42" alt="" /> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-19.png" width="45" height="42" alt="" /> = 46<img src="1739254504_matrix-theory/1739254504_matrix-theory-20.png" width="49" height="42" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>x</strong><strong><sub>5</sub></strong><strong> = Ax</strong><strong><sub>4 </sub></strong><strong>= </strong><img src="1739254504_matrix-theory/1739254504_matrix-theory-21.png" width="127" height="42" alt="" /> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-22.png" width="63" height="42" alt="" /> = -94<img src="1739254504_matrix-theory/1739254504_matrix-theory-23.png" width="49" height="42" alt="" />
			</p>
			<p>
				<strong>x</strong><strong><sub>6</sub></strong><strong> = Ax</strong><strong><sub>5 </sub></strong><strong>= </strong><img src="1739254504_matrix-theory/1739254504_matrix-theory-24.png" width="145" height="42" alt="" /> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-25.png" width="45" height="42" alt="" /> = 190<img src="1739254504_matrix-theory/1739254504_matrix-theory-26.png" width="49" height="42" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				Note that the approximations in Example appear to be approaching scalar multiples of <img src="1739254504_matrix-theory/1739254504_matrix-theory-27.png" width="24" height="42" alt="" />
			</p>
			<p>
				So, the obtained dominant eigenvector from the above iterations is
			</p>
			<p>
				<strong>x</strong> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-27.png" width="24" height="42" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				Now, we’ll find the corresponding eigenvalue from the obtained eigenvector
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Formula</strong>
			</p>
			<p>
				If <strong>x </strong>is an eigenvector of <strong>A</strong>, then its corresponding eigenvalue is given by 
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				λ = (<strong>Ax.x / x.x</strong>)
			</p>
			<p>
				<strong>Ax </strong>= <img src="1739254504_matrix-theory/1739254504_matrix-theory-28.png" width="130" height="42" alt="" /> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-29.png" width="67" height="42" alt="" />
			</p>
			<p>
				Then, <strong>Ax.x = </strong><img src="1739254504_matrix-theory/1739254504_matrix-theory-30.png" width="116" height="42" alt="" /> = -20.0 (approx.)
			</p>
			<p>
				And <strong>x.x </strong>= <img src="1739254504_matrix-theory/1739254504_matrix-theory-31.png" width="97" height="42" alt="" /> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-26.png" width="49" height="42" alt="" /> = 9.94 (approx.)
			</p>
			<p>
				So, the corresponding eigenvalue, λ = (-20.0 / 9.94) = -2 (approx.)
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Singular Value Decomposition (SVD)</strong>
			</p>
			<p>
				<strong>Theory:</strong>
			</p>
			<p>
				Singular Value Decomposition (SVD) is a matrix factorization technique that decomposes any m×n matrix A into three matrices: A=UΣV<sup>T</sup> 
			</p>
			<p>
				Where:
			</p>
			<ul>
				<li>
					U is an m×m orthogonal matrix (or unitary if complex).
				</li>
				<li>
					Σ is m×n diagonal matrix with non-negative real numbers on the diagonal (singular values).
				</li>
				<li>
					V<sup>T</sup> is an n×n orthogonal matrix (or unitary if complex), and V<sup>T</sup> is the transpose of V.
				</li>
			</ul>
			<p>
				<strong>Example</strong>
			</p>
			<p>
				A = <img src="1739254504_matrix-theory/1739254504_matrix-theory-32.png" width="53" height="42" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				Compute A<sup>T</sup>A and AA<sup>T</sup>
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				A<sup>T</sup>A = <img src="1739254504_matrix-theory/1739254504_matrix-theory-33.png" width="111" height="42" alt="" /> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-34.png" width="74" height="42" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				AA<sup>T</sup> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-35.png" width="111" height="42" alt="" /> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-36.png" width="74" height="42" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Find Eigenvalues and Eigenvectors</strong>:
			</p>
			<p>
				For A<sup>T</sup>A:
			</p>
			<ul>
				<li>
					Eigenvalues are λ1= 29.8661 and λ2 = 0.1339
				</li>
				<li>
					Corresponding eigenvectors (normalized) are v1 = <img src="1739254504_matrix-theory/1739254504_matrix-theory-37.png" width="92" height="42" alt="" />and v2 = <img src="1739254504_matrix-theory/1739254504_matrix-theory-38.png" width="88" height="42" alt="" />
				</li>
			</ul>
			<p>
				For AA<sup>T</sup>:
			</p>
			<ul>
				<li>
					Eigenvalues are λ1= 29.8661 and λ2 = 0.1339
				</li>
				<li>
					Corresponding eigenvectors (normalized) are u1 = <img src="1739254504_matrix-theory/1739254504_matrix-theory-39.png" width="92" height="42" alt="" />and u2 = <img src="1739254504_matrix-theory/1739254504_matrix-theory-40.png" width="88" height="42" alt="" />
				</li>
			</ul>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Compute Singular Values</strong>:
			</p>
			<p>
				Singular values are σ1 = √ 29.8661= 5.4650
			</p>
			<p>
				And σ1 = √ 0.1339 = 0.3660
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Final SVD:</strong>
			</p>
			<p>
				A=UΣV<sup>T</sup>
			</p>
			<p>
				Where: 
			</p>
			<p>
				U = <img src="1739254504_matrix-theory/1739254504_matrix-theory-41.png" width="185" height="42" alt="" />&#xa0; Σ = <img src="1739254504_matrix-theory/1739254504_matrix-theory-42.png" width="149" height="42" alt="" />&#xa0; 
			</p>
			<p>
				V<sup>T</sup> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-43.png" width="180" height="42" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>LU Decomposition</strong>
			</p>
			<p>
				LU Decomposition is a method to find solutions of linear equations.
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				Using Gauss Elimination Method 
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				Consider a matrix&#xa0;𝐴. If all entries below the diagonal entries are zero, then the matrix is called “upper triangular.” If all entries above the diagonal entries are zero, then the matrix is called “lower triangular.” 
			</p>
			<p>
				&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0; And <strong>A = L*U</strong>
			</p>
			<p>
				<strong>L</strong> =<img src="1739254504_matrix-theory/1739254504_matrix-theory-44.png" width="201" height="79" alt="" />&#xa0; ;&#xa0;&#xa0; <strong>U</strong> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-45.png" width="178" height="79" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong><em>L= lower triangular matrix; U= upper triangular matrix</em></strong>
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Procedure-</strong>
			</p>
			<ol>
				<li>
					Choose a matrix (<strong>m X n) </strong>(e.g., 3X 3, 3 X 4, 4 X 4, etc.,)
				</li>
			</ol>
			<p>
				&#xa0;
			</p>
			<ol start="2">
				<li>
					Initialize the <strong>L</strong> and <strong>U </strong>matrices. For L matrix, take a matrix with all diagonal elements assigned to 1, and the remaining components are zero. L matrix size will be (<strong>m X m</strong>). 
				</li>
			</ol>
			<p>
				&#xa0;
			</p>
			<ol start="3">
				<li>
					Consider the <strong>U</strong> matrix's elements to be same to those of the <strong>A</strong> matrix. So, size of matrix <strong>U </strong>will be as same as matrix<strong> A</strong>.
				</li>
			</ol>
			<p>
				&#xa0;
			</p>
			<p>
				&#xa0;
			</p>
			<ol start="4">
				<li>
					4. Next, execute row operations on the <strong>U</strong> matrix to make sure that all of the components below the diagonal are zeroes. For instance, to make an element in row 2 or R2 at the (i,j)th position zero, we would first do 
				</li>
			</ol>
			<p>
				&#xa0;
			</p>
			<p>
				"R2 - (-2)*R1" 
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				and then set the value ‘(-2)’ at the (i,j)th place of the L Matrix.
			</p>
			<p>
				&#xa0;
			</p>
			<ol start="5">
				<li>
					For a given matrix
				</li>
			</ol>
			<p>
				<strong>A</strong> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-46.png" width="186" height="79" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>A=L*U</strong>
			</p>
			<p>
				Or, <strong>A</strong>=<img src="1739254504_matrix-theory/1739254504_matrix-theory-47.png" width="117" height="79" alt="" />* <img src="1739254504_matrix-theory/1739254504_matrix-theory-46.png" width="186" height="79" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong><em>L= lower triangular matrix; U= upper triangular matrix</em></strong>
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				After doing the row&#xa0;operation "R2 - (-2)*R1," we get,
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				Or, <strong>A</strong> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-48.png" width="135" height="79" alt="" />* <img src="1739254504_matrix-theory/1739254504_matrix-theory-49.png" width="142" height="79" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				After row operation in matrix <strong>U</strong>, we've set (-2) to the same place of the<strong> L</strong> matrix and the (2, 1)<sup>th</sup> position of the U matrix, which is now zero.
			</p>
			<p>
				Firstly, try the first column elements of matrix <strong>U</strong> below diagonal elements to make zeroes,
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				After doing,&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0; ‘R2-(-2)*R1’ <em>(as demonstrated above)</em>
			</p>
			<p>
				&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0; ‘R3-(3)*R1’ and
			</p>
			<p>
				&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0; ‘R4-(2)*R1’
			</p>
			<p>
				We get,
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>A</strong> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-50.png" width="135" height="79" alt="" />* <img src="1739254504_matrix-theory/1739254504_matrix-theory-51.png" width="167" height="79" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				In a same way, we will now employ row operations to set the elements of the second column of matrix <strong>U</strong> to zero.
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				Calculate,&#xa0;&#xa0;&#xa0; ‘R3-(-4)*R2’ and
			</p>
			<p>
				&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0; ‘R4-(1)*R2’
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				We get,
			</p>
			<p>
				<strong>A</strong> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-52.png" width="153" height="79" alt="" />* <img src="1739254504_matrix-theory/1739254504_matrix-theory-53.png" width="149" height="79" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				Now, we'll apply row operations to convert the elements of the third column of matrix <strong>U</strong> to zeroes.
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				Now calculate "R4-(3)*R2"
			</p>
			<p>
				We get,
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>A</strong> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-54.png" width="153" height="79" alt="" />* <img src="1739254504_matrix-theory/1739254504_matrix-theory-55.png" width="149" height="79" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				So, 
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>L</strong> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-54.png" width="153" height="79" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>U</strong> = <img src="1739254504_matrix-theory/1739254504_matrix-theory-55.png" width="149" height="79" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<ol start="6">
				<li>
					<strong>Before showing the final result, all intermediate steps must be displayed.</strong>
				</li>
			</ol>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Row Echelon Form</strong>
			</p>
			<p>
				A matrix is in row echelon form if
			</p>
			<ul>
				<li>
					All rows consisting of only zeroes are at the bottom.
				</li>
				<li>
					The leading entry (that is the left-most nonzero entry) of every nonzero row is to the right of the leading entry of every row above
				</li>
				<li>
					Some texts add the condition that the leading coefficient must be 1 while others regard this as reduced row echelon form
				</li>
				<li>
					These two conditions imply that all entries in a column below a leading coefficient are zeros
				</li>
			</ul>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Procedure</strong>
			</p>
			<ol>
				<li>
					Choose an m X n matrix
				</li>
				<li>
					All zero rows are at the bottom. 
				</li>
				<li>
					Choose the leading entry in the first non-zero row and swap it with the first row if necessary. Or, the leading entry/element in the first row must be non-zero.
				</li>
				<li>
					Divide the first row by the leading entry so that the leading entry becomes 1.
				</li>
				<li>
					Use row operations to make all entries in the first column below the leading entry equal to 0.
				</li>
				<li>
					Repeat steps 3 through 5 for each subsequent row, working from top to bottom.
				</li>
			</ol>
			<p>
				&#xa0;
			</p>
			<p>
				These conditions also imply that all entries in a column below a leading coefficient are zeros
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Example</strong>
			</p>
			<p>
				Given matrix,
			</p>
			<p>
				<strong>A = </strong><img src="1739254504_matrix-theory/1739254504_matrix-theory-56.png" width="104" height="61" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				R2 ← R2 – R1
			</p>
			<p>
				<img src="1739254504_matrix-theory/1739254504_matrix-theory-57.png" width="100" height="61" alt="" />
			</p>
			<p>
				<strong>&#xa0;</strong>
			</p>
			<p>
				R3 ← R3 + 2*R2
			</p>
			<p>
				<img src="1739254504_matrix-theory/1739254504_matrix-theory-58.png" width="100" height="61" alt="" />
			</p>
			<p>
				<strong>&#xa0;</strong>
			</p>
			<p>
				R3 ← R3 / 4
			</p>
			<p>
				<img src="1739254504_matrix-theory/1739254504_matrix-theory-59.png" width="100" height="61" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Reduced Row Echelon Form (RREF)</strong>
			</p>
			<p>
				<strong>Procedure</strong>
			</p>
			<ol>
				<li>
					Choose an m X n matrix
				</li>
				<li>
					All zero rows are at the bottom. 
				</li>
				<li>
					Choose the leading entry in the first non-zero row and swap it with the first row if necessary.
				</li>
				<li>
					Divide the first row by the leading entry so that the leading entry becomes 1.
				</li>
				<li>
					Use row operations to make all entries in the first column above and below the leading entry equal to 0.
				</li>
				<li>
					Repeat steps 3 through 5 for each subsequent row, working from top to bottom.
				</li>
				<li>
					After all, rows have been processed, the matrix is in reduced row echelon form.
				</li>
			</ol>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Example:</strong>
			</p>
			<p>
				Given matrix <strong>A </strong>= <img src="1739254504_matrix-theory/1739254504_matrix-theory-60.png" width="94" height="61" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				R2 ← R2 – 2*R1 <em>(R1 denotes row 1 and so on)</em>
			</p>
			<p>
				<img src="1739254504_matrix-theory/1739254504_matrix-theory-61.png" width="94" height="61" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				R3 ← R3 – 3*R1
			</p>
			<p>
				<img src="1739254504_matrix-theory/1739254504_matrix-theory-62.png" width="83" height="61" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				R1 ← R1 – 2*R2
			</p>
			<p>
				<img src="1739254504_matrix-theory/1739254504_matrix-theory-63.png" width="83" height="61" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				R1 ← R1 – R3
			</p>
			<p>
				<img src="1739254504_matrix-theory/1739254504_matrix-theory-64.png" width="83" height="61" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				R2 ← R2 – R3
			</p>
			<p>
				<img src="1739254504_matrix-theory/1739254504_matrix-theory-65.png" width="83" height="61" alt="" />
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Rank of a Matrix</strong>
			</p>
			<p>
				<strong>Theory:</strong>
			</p>
			<p>
				<strong>Definition</strong>: The rank of a matrix is defined as the maximum number of linearly independent rows (or columns) in the matrix. It can also be seen as the dimension of the row space or column space of the matrix.
			</p>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Rank of a Matrix in Row Echelon Form (REF)</strong>
			</p>
			<ol>
				<li>
					<strong>Row Echelon Form (REF)</strong>: A matrix is in row echelon form when:
					<ul>
						<li>
							All non-zero rows are above any rows of all zeros.
						</li>
						<li>
							The leading entry (pivot) of each non-zero row is to the right of the leading entry of the row above it.
						</li>
						<li>
							All entries below a pivot are zero.
						</li>
					</ul>
				</li>
			</ol>
			<p>
				&#xa0;
			</p>
			<ol start="2">
				<li>
					<strong>Finding the Rank</strong>:
					<ul>
						<li>
							<strong>Identify Non-Zero Rows</strong>: In REF, the rank of the matrix is equal to the number of non-zero rows. This is because each non-zero row represents a linearly independent vector in the row space of the matrix.
						</li>
						<li>
							<strong>Process</strong>: Convert the matrix to REF using row operations (row swapping, scaling rows, adding/subtracting multiples of rows) and count the number of non-zero rows to determine the rank.
						</li>
					</ul>
				</li>
			</ol>
			<p>
				&#xa0;
			</p>
			<p>
				<strong>Rank of a Matrix in Reduced Row Echelon Form (RREF)</strong>
			</p>
			<p>
				<strong>Theory:</strong>
			</p>
			<ol>
				<li>
					<strong>Reduced Row Echelon Form (RREF)</strong>: A matrix is in reduced row echelon form when:
					<ul>
						<li>
							It is in row echelon form (REF).
						</li>
						<li>
							Each leading entry (pivot) is 1.
						</li>
						<li>
							Each leading 1 is the only non-zero entry in its column.
						</li>
						<li>
							All rows with leading 1s are above rows of all zeros.
						</li>
					</ul>
				</li>
			</ol>
			<p>
				&#xa0;
			</p>
			<ol start="2">
				<li>
					<strong>Finding the Rank</strong>:
					<ul>
						<li>
							<strong>Count Leading 1s</strong>: In RREF, the rank of the matrix is equal to the number of leading 1s. Each leading 1 represents a pivot position in a linearly independent row.
						</li>
						<li>
							<strong>Process</strong>: Convert the matrix to RREF using row operations (pivoting, scaling, and clearing entries above and below pivots) and count the number of leading 1s to determine the rank.
						</li>
					</ul>
				</li>
			</ol>
 
</body>
</html>
>>>>>>> bfb9e6c (Dev Files)
